
# ğŸš€ AI Workshop â€” Build a Transformer From Scratch

This workshop guides you through implementing core transformer components from first principles using Python and Jupyter.

Youâ€™ll set up a reproducible environment, install dependencies, and run interactive notebooks to understand how modern LLMs actually work internally.

---

# ğŸ“‚ Project Structure

```
ai_workshop/
â”‚
â”œâ”€â”€ data/               # Dataset (auto-downloaded)
â”œâ”€â”€ day/                # Notebooks
â”œâ”€â”€ material/           # Theory PDFs
â”œâ”€â”€ reference/          # Reference implementations
â”‚
â”œâ”€â”€ linux/              # Fedora setup scripts
â”‚   â”œâ”€â”€ setup.sh
â”‚   â”œâ”€â”€ verify.sh
â”‚   â””â”€â”€ reset.sh
â”‚
â”œâ”€â”€ windows/            # Windows setup scripts
â”‚   â”œâ”€â”€ setup.ps1
â”‚   â”œâ”€â”€ verify.ps1
â”‚   â””â”€â”€ reset.ps1
â”‚
â””â”€â”€ README.md
```

---

# ğŸ§ Fedora Setup

## Requirements

* Python **3.11 or newer**
* Internet connection

Check version:

```bash
python3 --version
```

If missing:

```bash
sudo dnf install python3 python3-pip
```

---

## Run Setup

From project root:

```bash
chmod +x linux/setup.sh
./linux/setup.sh
```

This automatically:

* installs `uv`
* creates virtual environment
* installs dependencies
* downloads dataset
* verifies everything
* launches Jupyter Notebook

If setup succeeds, youâ€™re ready.

---

## Manual Verify (optional)

```bash
./linux/verify.sh
```

---

## Reset Environment

If something breaks:

```bash
./linux/reset.sh
./linux/setup.sh
```

---

# ğŸªŸ Windows Setup

âš  Use **PowerShell**, not Command Prompt.

---

## Requirements

Install Python 3.11+ from:

[https://www.python.org/downloads/](https://www.python.org/downloads/)

During install:

âœ” Check **Add Python to PATH**

Verify:

```powershell
python --version
```

---

## Run Setup

From project root:

```powershell
powershell -ExecutionPolicy Bypass -File windows/setup.ps1
```

This automatically:

* installs uv
* creates environment
* installs packages
* downloads dataset
* verifies setup

---

## Manual Verify

```powershell
powershell -ExecutionPolicy Bypass -File windows/verify.ps1
```

---

## Reset Environment

```powershell
powershell -ExecutionPolicy Bypass -File windows/reset.ps1
```

Then rerun setup.

---

# ğŸ“Š Dataset

Downloaded automatically to:

```
data/names.txt
```

Used for character-level transformer experiments.

---

# â–¶ Running Notebook

After setup finishes:

```
jupyter notebook
```

Open:

```
day/day1.ipynb
```

---

# ğŸ›  Troubleshooting

### Python version unsupported

Supported versions:

* 3.11
* 3.12
* 3.14

Install correct version â†’ reset â†’ setup again.

---

### Graphviz visualization errors

Install system package:

**Fedora**

```
sudo dnf install graphviz
```

**Windows**
Install from:
[https://graphviz.org/download/](https://graphviz.org/download/)

Restart terminal afterward.

---

### Setup fails midway

Run reset, then setup again:

Linux

```
./linux/reset.sh
./linux/setup.sh
```

Windows

```
powershell -ExecutionPolicy Bypass -File windows/reset.ps1
powershell -ExecutionPolicy Bypass -File windows/setup.ps1
```

---

# ğŸ¯ Workshop Goal

By the end, youâ€™ll understand:

* how attention actually works
* why masking is needed
* how positional encoding functions
* what a transformer block really does internally

Not surface-level theory â€” actual implementation understanding.
